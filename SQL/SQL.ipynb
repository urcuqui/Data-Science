{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "declared-combine",
   "metadata": {},
   "source": [
    "The next material was taken from Kaggle. All credits are for the original authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT COUNT(ID)\n",
    "FROM table.pets\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-booth",
   "metadata": {},
   "source": [
    "AVG() SUM() MIN() MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT Animal, COUNT(ID)\n",
    "FROM table.pets\n",
    "GROUP BY Animal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-oxygen",
   "metadata": {},
   "source": [
    "*aggregate functionS*\n",
    "\n",
    "**Having** is used in combination with **GROUP BY** to ignore groups that do not meet certain criteria.\n",
    "\n",
    "For example the next query will only include groups that have more than one ID in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT Animal, COUNT(ID)\n",
    "FROM table.pets\n",
    "GROUP BY Animal\n",
    "HAVING COUNT(ID) > 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "instructional-minister",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-datastore\n",
      "  Downloading google_cloud_datastore-2.1.6-py2.py3-none-any.whl (127 kB)\n",
      "Collecting proto-plus>=1.4.0\n",
      "  Downloading proto_plus-1.19.0-py3-none-any.whl (42 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.0\n",
      "  Downloading google_cloud_core-2.0.0-py2.py3-none-any.whl (27 kB)\n",
      "Collecting google-api-core[grpc]<3.0.0dev,>=1.22.2\n",
      "  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
      "Collecting libcst>=0.2.5\n",
      "  Downloading libcst-0.3.21-py3-none-any.whl (514 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (2.25.1)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (52.0.0.post20210125)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (1.27.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (3.14.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (1.36.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (1.15.0)\n",
      "Requirement already satisfied: pyyaml>=5.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from libcst>=0.2.5->google-cloud-datastore) (5.4.1)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from libcst>=0.2.5->google-cloud-datastore) (3.7.4.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.22.2->google-cloud-datastore) (2021.5.30)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Installing collected packages: mypy-extensions, googleapis-common-protos, typing-inspect, google-api-core, proto-plus, libcst, google-cloud-core, google-cloud-datastore\n",
      "Successfully installed google-api-core-2.0.1 google-cloud-core-2.0.0 google-cloud-datastore-2.1.6 googleapis-common-protos-1.53.0 libcst-0.3.21 mypy-extensions-0.4.3 proto-plus-1.19.0 typing-inspect-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guilty-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-2.26.0-py2.py3-none-any.whl (201 kB)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.25.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (1.19.0)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (20.9)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.0.3-py2.py3-none-any.whl (75 kB)\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Downloading grpcio-1.40.0-cp37-cp37m-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.0.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (3.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.53.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.27.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (52.0.0.post20210125)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (4.2.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (0.2.8)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.2.0-cp37-cp37m-win_amd64.whl (27 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from packaging>=14.3->google-cloud-bigquery) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2.10)\n",
      "Installing collected packages: grpcio, google-crc32c, google-resumable-media, google-cloud-bigquery\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.36.0\n",
      "    Uninstalling grpcio-1.36.0:\n",
      "      Successfully uninstalled grpcio-1.36.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.1.0 requires scipy==1.4.1; python_version >= \"3\", but you have scipy 1.6.1 which is incompatible.\n",
      "tensorflow 2.1.0 requires tensorboard<2.2.0,>=2.1.0, but you have tensorboard 2.3.0 which is incompatible.\n",
      "econml 0.7.0 requires scikit-learn~=0.21.0, but you have scikit-learn 0.24.1 which is incompatible.\n",
      "econml 0.7.0 requires tensorflow==1.*, but you have tensorflow 2.1.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully installed google-cloud-bigquery-2.26.0 google-crc32c-1.2.0 google-resumable-media-2.0.3 grpcio-1.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-robert",
   "metadata": {},
   "source": [
    "If you are ever ensure what to put inside COUNT() function, you can do COUNT(1) to count the rows in each group. Less data quota using a cloud provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT parent, COUNT(1) AS NumPosts\n",
    "FROM comments\n",
    "GROUP BY parent\n",
    "HAVING COUNT(1) > 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-vegetation",
   "metadata": {},
   "source": [
    "IF YOU HAVE ANY **GROUP BY** CLAUSE, then all variables must be passed to either a without an aggregate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-stephen",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "### 1) Prolific commenters\n",
    "\n",
    "Hacker News would like to send awards to everyone who has written more than 10,000 posts. Write a query that returns all authors with more than 10,000 posts as well as their post counts. Call the column with post counts `NumPosts`.\n",
    "\n",
    "In case sample query is helpful, here is a query you saw in the tutorial to answer a similar question:\n",
    "```\n",
    "query = \"\"\"\n",
    "        SELECT parent, COUNT(1) AS NumPosts\n",
    "        FROM `bigquery-public-data.hacker_news.comments`\n",
    "        GROUP BY parent\n",
    "        HAVING COUNT(1) > 10\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT author, COUNT(1) AS NumPosts\n",
    "FROM `bigquery-public-data.hacker_news.comments`\n",
    "GROUP BY author\n",
    "HAVING COUNT(1) > 10000\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-curve",
   "metadata": {},
   "source": [
    "### 2) Deleted comments\n",
    "\n",
    "How many comments have been deleted? (If a comment was deleted, the `deleted` column in the comments table will have the value `True`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT COUNT(1) \n",
    "FROM `bigquery-public-data.hacker_news.comments`\n",
    "WHERE deleted = True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-angola",
   "metadata": {},
   "source": [
    "**ORDER BY** is usually the LAST clause in your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT ID, Name, Animal\n",
    "FROM pets\n",
    "ORDER BY ID\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it also works in columns with categorical values, it is going to sort it using an alpabethic approach\n",
    "\"\"\"\n",
    "SELECT ID, Name, Animal\n",
    "FROM pets\n",
    "ORDER BY Animal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-climb",
   "metadata": {},
   "source": [
    "We can also use **DESC** to change the order if our sorting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT ID, Name, Animal\n",
    "FROM pets\n",
    "ORDER BY Animal DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-minority",
   "metadata": {},
   "source": [
    "**DATES**\n",
    "\n",
    "We have two ways that dates can be stored in a database, **DATE** and **DATETIME**\n",
    "\n",
    "he DATE format has the year first, then the month, and then the day. It looks like this:\n",
    "\n",
    "YYYY-[M]M-[D]D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-sweden",
   "metadata": {},
   "source": [
    "*EXTRACT* is the way to get a part of a date, like the year or the day. \n",
    "\n",
    "Meaning\t| extract field\n",
    "\n",
    "Year\t| YEAR\n",
    "\n",
    "Month\t|MONTH\n",
    "\n",
    "Day of month\t|DAY\n",
    "\n",
    "24 hour\t|HOUR\n",
    "\n",
    "Minute\t|MINUTE\n",
    "\n",
    "Seconds (including fractions)\t|SECOND\n",
    "\n",
    "Time zone hour\t|TIMEZONE_HOUR\n",
    "\n",
    "Time zone minute\t|TIMEZONE_MINUTE\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/vhvHIh0.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT Name, EXTRACT(DAY from Date) as Day\n",
    "FROM pets_with_date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT Name, EXTRACT(WEEK from Date) as WEEK}\n",
    "FROM pets_with_date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-board",
   "metadata": {},
   "source": [
    "Example: *Which day of the week has the most fatal motor accidents?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT EXTRACT(DAY FROM timestamp_of_crash) as day, COUNT(consecutive_number) as number_accidents\n",
    "FROM accidents\n",
    "GROUP BY day\n",
    "ORDER BY number_accidents DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-mississippi",
   "metadata": {},
   "source": [
    "Exercises\n",
    "\n",
    "The value in the `indicator_code` column describes what type of data is shown in a given row.  \n",
    "\n",
    "One interesting indicator code is `SE.XPD.TOTL.GD.ZS`, which corresponds to \"Government expenditure on education as % of GDP (%)\".\n",
    "\n",
    "### 1) Government expenditure on education\n",
    "\n",
    "Which countries spend the largest fraction of GDP on education?  \n",
    "\n",
    "To answer this question, consider only the rows in the dataset corresponding to indicator code `SE.XPD.TOTL.GD.ZS`, and write a query that returns the average value in the `value` column for each country in the dataset between the years 2010-2017 (including 2010 and 2017 in the average). \n",
    "\n",
    "Requirements:\n",
    "- Your results should have the country name rather than the country code. You will have one row for each country.\n",
    "- The aggregate function for average is **AVG()**.  Use the name `avg_ed_spending_pct` for the column created by this aggregation.\n",
    "- Order the results so the countries that spend the largest fraction of GDP on education show up first.\n",
    "\n",
    "In case it's useful to see a sample query, here's a query you saw in the tutorial (using a different dataset):\n",
    "```\n",
    "# Query to find out the number of accidents for each day of the week\n",
    "query = \"\"\"\n",
    "        SELECT COUNT(consecutive_number) AS num_accidents, \n",
    "               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n",
    "        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
    "        GROUP BY day_of_week\n",
    "        ORDER BY num_accidents DESC\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT contry_name, AVG(value) AS avg_ed_spending_pct, year\n",
    "FROM international_education\n",
    "WHERE indicator_name = \"SE.XPD.TOTL.GD.ZS\" AND year >= 2010 AND year <= 2017\n",
    "GROUP BY coutry_code\n",
    "ORDER BY avg_ed_spending_pct DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-enhancement",
   "metadata": {},
   "source": [
    "### 2) Identify interesting codes to explore\n",
    "\n",
    "The last question started by telling you to focus on rows with the code `SE.XPD.TOTL.GD.ZS`. But how would you find more interesting indicator codes to explore?\n",
    "\n",
    "There are 1000s of codes in the dataset, so it would be time consuming to review them all. But many codes are available for only a few countries. When browsing the options for different codes, you might restrict yourself to codes that are reported by many countries.\n",
    "\n",
    "Write a query below that selects the indicator code and indicator name for all codes with at least 175 rows in the year 2016.\n",
    "\n",
    "Requirements:\n",
    "- You should have one row for each indicator code.\n",
    "- The columns in your results should be called `indicator_code`, `indicator_name`, and `num_rows`.\n",
    "- Only select codes with 175 or more rows in the raw database (exactly 175 rows would be included).\n",
    "- To get both the `indicator_code` and `indicator_name` in your resulting DataFrame, you need to include both in your **SELECT** statement (in addition to a **COUNT()** aggregation). This requires you to include both in your **GROUP BY** clause.\n",
    "- Order from results most frequent to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT indicator_code, indicator_name, COUNT(1) AS num_rows\n",
    "FROM international_education\n",
    "WHERE year = 2016\n",
    "GROUP BY indicator_code, indicator_name\n",
    "HAVING COUNT(1) >= 175\n",
    "ORDER BY indicator_code DESC\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-nudist",
   "metadata": {},
   "source": [
    "**WITH** in what's called a \"common table expresssion\" is a temporary table that you return within your query. CTEs are helpful for spliting your queries into readable chunks, and you can write queries against them.\n",
    "\n",
    "For example,\n",
    "\n",
    "you might want to use the pets table to ask questions about older animals in particular. So you can start by creating a CTE which only contains information about animals more than five years old like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WITH Seniors AS\n",
    "(\n",
    "    SELECT ID, Name\n",
    "    FROM pets\n",
    "    WHERE Years_old > 5\n",
    "\n",
    ")\n",
    "SELECT ID\n",
    "FROM Seniors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-championship",
   "metadata": {},
   "source": [
    "We're going to use a CTE to find out how many Bitcoin transactions were made each day for the entire timespan of a bitcoin transaction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WITH time AS\n",
    "(\n",
    "    SELECT DATE(block_timestamp) AS trans_date\n",
    "    FROM transactions\n",
    ")\n",
    "SELECT trans_date, COUNT(1) as transactions\n",
    "FROM time\n",
    "GROUP BY trans_date\n",
    "ORDER BY trans_date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-western",
   "metadata": {},
   "source": [
    "### 3) Determine when this data is from\n",
    "\n",
    "If the data is sufficiently old, we might be careful before assuming the data is still relevant to traffic patterns today. Write a query that counts the number of trips in each year.  \n",
    "\n",
    "Your results should have two columns:\n",
    "- `year` - the year of the trips\n",
    "- `num_trips` - the number of trips in that year\n",
    "\n",
    "Hints:\n",
    "- When using **GROUP BY** and **ORDER BY**, you should refer to the columns by the alias `year` that you set at the top of the **SELECT** query.\n",
    "- The SQL code to **SELECT** the year from `trip_start_timestamp` is <code>SELECT EXTRACT(YEAR FROM trip_start_timestamp)</code>\n",
    "- The **FROM** field can be a little tricky until you are used to it.  The format is:\n",
    "    1. A backick (the symbol \\`).\n",
    "    2. The project name. In this case it is `bigquery-public-data`.\n",
    "    3. A period.\n",
    "    4. The dataset name. In this case, it is `chicago_taxi_trips`.\n",
    "    5. A period.\n",
    "    6. The table name. You used this as your answer in **1) Find the data**.\n",
    "    7. A backtick (the symbol \\`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WITH time AS\n",
    "(\n",
    "    SELECT EXTRACT(YEAR from trip_start_timestamp) as year\n",
    "    FROM chicago_taxi_trips\n",
    ")\n",
    "SELECT year, COUNT(1) AS num_trips\n",
    "FROM time\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-painting",
   "metadata": {},
   "source": [
    "### 4) Dive slightly deeper\n",
    "\n",
    "You'd like to take a closer look at rides from 2017.  Copy the query you used above in `rides_per_year_query` into the cell below for `rides_per_month_query`.  Then modify it in two ways:\n",
    "1. Use a **WHERE** clause to limit the query to data from 2017.\n",
    "2. Modify the query to extract the month rather than the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WITH time AS\n",
    "(\n",
    "    SELECT EXTRACT(MONTH from trip_start_timestamp) AS month\n",
    "    FROM chicago_taxi_trips\n",
    ")\n",
    "SELECT month, COUNT(1)\n",
    "FROM time\n",
    "WHERE month = 2017\n",
    "GROUP BY month\n",
    "ORDER BY month\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-prison",
   "metadata": {},
   "source": [
    "\n",
    "It's time to step up the sophistication of your queries.  Write a query that shows, for each hour of the day in the dataset, the corresponding number of trips and average speed.\n",
    "\n",
    "Your results should have three columns:\n",
    "- `hour_of_day` - sort by this column, which holds the result of extracting the hour from `trip_start_timestamp`.\n",
    "- `num_trips` - the count of the total number of trips in each hour of the day (e.g. how many trips were started between 6AM and 7AM, independent of which day it occurred on).\n",
    "- `avg_mph` - the average speed, measured in miles per hour, for trips that started in that hour of the day.  Average speed in miles per hour is calculated as `3600 * SUM(trip_miles) / SUM(trip_seconds)`. (The value 3600 is used to convert from seconds to hours.)\n",
    "\n",
    "Restrict your query to data meeting the following criteria:\n",
    "- a `trip_start_timestamp` between **2017-01-01** and **2017-07-01**\n",
    "- `trip_seconds` > 0 and `trip_miles` > 0\n",
    "\n",
    "You will use a common table expression (CTE) to select just the relevant rides.  Because this dataset is very big, this CTE should select only the columns you'll need to create the final output (though you won't actually create those in the CTE -- instead you'll create those in the later **SELECT** statement below the CTE).\n",
    "\n",
    "This is a much harder query than anything you've written so far.  Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WITH time as \n",
    "(\n",
    "    SELECT EXTRACT(HOUR from trip_start_timestamp) AS hour_of_day, trip_miles, trip_seconds\n",
    "    FROM taxi_trips\n",
    "    WHERE trip_start_timestamp > '2017-01-01' AND\n",
    "    trip_start_timestamp <  '2017-07-01' AND \n",
    "    tip_seconds > 0 AND\n",
    "    trip_miles > 0\n",
    ")\n",
    "SELECT hour_of_day, COUNT(1) AS num_trips, 3600 * SUM(trip_miles) / SUM(trip_seconds) AS avg_mph\n",
    "FROM time\n",
    "GROUP BY hour_of_day \n",
    "ORDER BY hour_of_day \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-affairs",
   "metadata": {},
   "source": [
    "**JOIN** we can create a table with just two columns the name of the pet and the name of the owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join\n",
    "\"\"\"\n",
    "SELECT p.Name AS Pet_Name, O.Name AS Owner_Name\n",
    "FROM pets AS p\n",
    "INNER JOIN owners AS o\n",
    "ON p.ID = o.PET_ID\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to determine the number of files per license, sorted by number of files\n",
    "\"\"\"\n",
    "SELECT l.license, COUNT(1) as number_of_files\n",
    "FROM sample_files AS s \n",
    "INNER JOIN licenses AS l \n",
    "    ON s.repo_name = l.repo_name\n",
    "GROUP BY l.license\n",
    "ORDER BY number_of_files DESC\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-courtesy",
   "metadata": {},
   "source": [
    "**WHERE** and the use of **LIKE** in text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT *\n",
    "FROM pets\n",
    "WHERE Name LIKE \"Ripley\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-ferry",
   "metadata": {},
   "source": [
    "We can also use % as a \"wildcard\" for any number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT *\n",
    "FROM pets\n",
    "WHERE Name LIKE \"%ipl%\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WITH questions AS\n",
    "(\n",
    "    SELECT id\n",
    "    FROM posts_questions\n",
    "    WHERE tags LIKE '%bigquery%'\n",
    ")\n",
    "SELECT a.id, a.body, a.owner_user_id\n",
    "FROM post_answers AS a\n",
    "INNER JOIN questions AS q\n",
    "ON a.parent_id = q.id\n",
    "WHERE tags LIKE '%bigquery%'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-edmonton",
   "metadata": {},
   "source": [
    "**JOINS**\n",
    "\n",
    "<img src=\"https://i.imgur.com/1Dvmg8S.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-christian",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/oa6VDig.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-auction",
   "metadata": {},
   "source": [
    "The query below pulls information from the stories and comments tables to create a table showing all stories posted on January 1, 2012, along with the corresponding number of comments. We use a LEFT JOIN so that the results include stories that didn't receive any comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select all stories posted on January 1, 2012, with number of comments\n",
    "\"\"\"\n",
    "WITH c AS\n",
    "(\n",
    "    SELECT parent, COUNT(*) AS num_comments\n",
    "    FROM comments\n",
    "    GROUP BY parent\n",
    ")\n",
    "SELECT\n",
    "FROM stores AS s\n",
    "LEFT JOIN c\n",
    "ON s.id = c.parent\n",
    "WHERE EXTRACT(DATE from s.time_ts) = '2012-01-01'\n",
    "ORDER BY c.num_comments DESC\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-enhancement",
   "metadata": {},
   "source": [
    "Next, we write a query to select all usernames corresponding to users who wrote stories or comments on January 1, 2014. We use UNION DISTINCT (instead of UNION ALL) to ensure that each user appears in the table at most once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT c.by\n",
    "FROM comments AS c\n",
    "WHERE EXTRACT(DATE from c.time_ts) = '2014-01-01'\n",
    "UNION DISTINCT\n",
    "SELECT s.by\n",
    "FROM stores AS s\n",
    "WHERE EXTRACT(DATE from s.time_ts) = '2014-01-01'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-state",
   "metadata": {},
   "source": [
    "SELECT DISTINCT column1, column2, ...\n",
    "\n",
    "FROM table_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " SELECT q.id AS q_id,\n",
    "                  MIN(TIMESTAMP_DIFF(a.creation_date, q.creation_date, SECOND)) as time_to_answer\n",
    "              FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "                  INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "              ON q.id = a.parent_id\n",
    "              WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'\n",
    "              GROUP BY q_id\n",
    "              ORDER BY time_to_answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                  SELECT q.owner_user_id \n",
    "                  FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "                  WHERE EXTRACT(DATE FROM q.creation_date) = '2019-01-01'\n",
    "                  UNION DISTINCT\n",
    "                  SELECT a.owner_user_id\n",
    "                  FROM `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "                  WHERE EXTRACT(DATE FROM a.creation_date) = '2019-01-01'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-basics",
   "metadata": {},
   "source": [
    "**Analytics functions** have an **OVER** clause, which defines the sets of rows used in each calculation. The **OVER** clause has three (optional) parts:\n",
    "\n",
    "+ The **PARTITION BY** clause divides the rows of the table into different groups. In the query above, we divide by id so that the calculations are separated by runner.\n",
    "+ The **ORDER BY** clause defines an ordering within each partition. In the sample query, ordering by the date column ensures that earlier training sessions appear first.\n",
    "+ The final clause (**ROWS BETWEEN 1 PRECEDING AND CURRENT ROW**) is known as a **window** frame clause. It identifies the set of rows used in each calculation. We can refer to this group of rows as a window. (Actually, analytic functions are sometimes referred to as analytic window functions or simply window functions!)\n",
    "\n",
    "<img src=\"https://i.imgur.com/GjiKlA7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-indian",
   "metadata": {},
   "source": [
    "*Window frame clauses*\n",
    "\n",
    "+ ROWS BETWEEN 1 PRECEDING AND CURRENT ROW - the previous row and the current row.\n",
    "+ ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING - the 3 previous rows, the current row, and the following row.\n",
    "+ ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING - all rows in the partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-spyware",
   "metadata": {},
   "source": [
    "## Three types of analytic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-harmony",
   "metadata": {},
   "source": [
    "1. Analytic aggregate functions\n",
    "As you might recall, AVG() (from the example above) is an aggregate function. The OVER clause is what ensures that it's treated as an analytic (aggregate) function. Aggregate functions take all of the values within the window as input and return a single value.\n",
    "\n",
    "    + MIN() (or MAX()) - Returns the minimum (or maximum) of input values\n",
    "    + AVG() (or SUM()) - Returns the average (or sum) of input values\n",
    "    + COUNT() - Returns the number of rows in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-melissa",
   "metadata": {},
   "source": [
    "2.  Analytic navigation functions\n",
    "Navigation functions assign a value based on the value in a (usually) different row than the current row.\n",
    "\n",
    "    + FIRST_VALUE() (or LAST_VALUE()) - Returns the first (or last) value in the input\n",
    "    + LEAD() (and LAG()) - Returns the value on a subsequent (or preceding) row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-grass",
   "metadata": {},
   "source": [
    "3. Analytic numbering functions\n",
    "Numbering functions assign integer values to each row based on the ordering.\n",
    "\n",
    "    + ROW_NUMBER() - Returns the order in which rows appear in the input (starting with 1)\n",
    "    + RANK() - All rows with the same value in the ordering column receive the same rank value, where the next row receives a rank value which increments by the number of rows with the previous rank value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each row of the table corresponds to a different bike trip, and we can use an analytic function to \n",
    "# calculate the cumulative number of trips for each date in 2015.\n",
    "\n",
    "\"\"\"\n",
    "WITH trips_by_day AS \n",
    "(\n",
    "    SELECT DATE(start_date) AS trip_date,\n",
    "    COUNT(*) AS num_trips\n",
    "    FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "    WHERE EXTRACT(YEAR from trip_date) = 2015\n",
    "    GROUP BY trip_date\n",
    ")\n",
    "SELECT *,\n",
    "    SUM(num_trips)\n",
    "        OVER(\n",
    "        ORDER BY trip_date\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW        \n",
    "        ) AS cumulative_trips\n",
    "    FROM trips_by_day\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-hands",
   "metadata": {},
   "source": [
    "The next query tracks the stations where each bike began (in start_station_id) and ended (in end_station_id) the day on October 25, 2015.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT bike_number\n",
    "    TIME(start_date) AS trip_date,\n",
    "    FIRST_VALUE(start_station_id)\n",
    "    OVER (\n",
    "        PARTITION BY bike_number\n",
    "        ORDER BY start_date\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) AS first_station_id\n",
    "    LAST_VALUE(end_statikon_id)\n",
    "    OVER (\n",
    "        PARTITION BY bike_number\n",
    "        ORDER BY start_date\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) AS last_station_id\n",
    "    start_station_id,\n",
    "    end_station_id\n",
    "    FROM bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "    WHERE DATE(start_date) = '2015-10-25'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-fields",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-webmaster",
   "metadata": {},
   "source": [
    "statistical queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "select \n",
    "\tdepartment_id, sum(salary), \n",
    "\tround(avg(salary),2), \n",
    "\tround(var_pop(salary),2), \n",
    "\tround(stddev_pop(salary),2)\n",
    "from\n",
    "\tdata_sci.employees\n",
    "group by department_id \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "select \n",
    "\tupper(department_name)\n",
    "from data_sci.company_departments\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the initial upper case\n",
    "\"\"\"\n",
    "select \tinitcap(department_name)from data_sci.company_departments\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spaces\n",
    "\n",
    "\"\"\"\n",
    "select \n",
    "\tltrim(' Kelly') = 'Kelly'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate two strings ||\n",
    "\"\"\"\n",
    "select \n",
    "\tjob_title || '-' || last_name\n",
    "from \n",
    "\tdata_sci.employees\n",
    "\"\"\"\n",
    "\n",
    "# PAY ATTENTION TO THE NULL VALUES USING ||\n",
    "\n",
    "\"\"\"\n",
    "select \n",
    "\tCONCAT(job_title, '-', last_name)\n",
    "from \n",
    "\tdata_sci.employees\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract substrings from strings\n",
    "\"\"\"\n",
    "select \n",
    "\tsubstring('abcdefghi', 1, 3) test_string\n",
    "\t\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "select \n",
    "\tsubstring('abcdefghi' from 1 for 3) test_string\n",
    "\t\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "select\n",
    "\tjob_title, (job_title like '%assistant%') is_assistant\n",
    "from\n",
    "\tdata_sci.employees\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter with regular expressions\n",
    "\"\"\"\n",
    "select distinct\n",
    "\tjob_title\n",
    "from \n",
    "\tdata_sci.employees\n",
    "where\n",
    "\tjob_title  like 'vp%'\n",
    "or\n",
    "\tjob_title  like 'web%'\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "select distinct\n",
    "\tjob_title\n",
    "from \n",
    "\tdata_sci.employees\n",
    "where\n",
    "\tjob_title similar to '(vp%|web%)'\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
