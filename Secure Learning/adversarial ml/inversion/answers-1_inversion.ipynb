{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0e1fa3-aec9-4e0c-a704-101a847e4add",
   "metadata": {},
   "source": [
    "# Inversion Answer Key\n",
    "This answer key is configured such that you should be able to run the code here and see possible approaches to a working solution. For each topic, it will also link further resources, and go into more detail on certain code chunks. It is not meant to be edited. \n",
    "\n",
    "Use these answer keys as a guide as needed. Try to work use the context here to work toward an answer before reaching for the solution.\n",
    "\n",
    "**If you just want to see the answers, they're all tagged with \"SOLUTION\", CTRL+F your heart out.**\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fa0e7-b33e-408e-891f-86d325a8fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.inference.model_inversion.mi_face import MIFace\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346157dc-f444-457e-9526-cde77a85d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.dense0 = nn.Linear(6272, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.convs(x)\n",
    "        \n",
    "        h = torch.flatten(h, 1)\n",
    "        h = self.dropout(h)\n",
    "        h = self.dense0(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c51c9-4351-42fe-a8e8-03abca31f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a randomly initialized model to have somewhere to put the weights\n",
    "model = MNIST_CNN_model()\n",
    "\n",
    "# torch.load loads the dictionary from file; we tell the model that the weights should be placed onto the cpu initially (otherwise\n",
    "# the device they were on when saved will be used by default)\n",
    "model.load_state_dict(torch.load(\"mnist_model.pt\", map_location='cpu'))\n",
    "# set the model into eval mode\n",
    "model.eval()\n",
    "# ... and move it to the correct device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88b388-c660-4973-b83d-44d7fc7ca970",
   "metadata": {},
   "source": [
    "## SOLUTION: Exercise - Using ART (MIFace on MNIST)\n",
    "### Resources\n",
    "- [MIFace Docs](https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/inference/model_inversion.html#model-inversion-miface)\n",
    "- [MIFace Paper](https://dl.acm.org/doi/pdf/10.1145/2810103.2813677)\n",
    "- [Example MIFace attacks](https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/model_inversion_attacks_mnist.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d338af1-deb9-42e2-a7f6-1d023f7d9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PyTorchClassifier(model=model, \n",
    "                               clip_values=[0,1],\n",
    "                               loss=F.cross_entropy,\n",
    "                               input_shape=(1,1,28,28),\n",
    "                               nb_classes=10\n",
    "                              )\n",
    "\n",
    "y = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "attack = MIFace(classifier, \n",
    "                max_iter=100,\n",
    "                learning_rate = 0.1\n",
    "               )\n",
    "\n",
    "x_train_infer = np.zeros((10,1,28,28))\n",
    "\n",
    "x_train_infer = attack.infer(x=x_train_infer, y=y)\n",
    "\n",
    "\n",
    "# plotting boilerplate\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(x_train_infer[i,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d579b49-20e6-4237-b476-1aae187654b7",
   "metadata": {},
   "source": [
    "_Help me understand..._\n",
    "- `clip_values=[0,1]`: We need to specify in our setup that the images resulting from the attack must be clipped between 0 and 1\n",
    "- `input_shape=(1,1,28,28)`: The classifier expects images with shape (B, C, H, W), aka 1 batch, 1 channel (black and white), 28x28 pixels.\n",
    "- `x_train_infer = np.zeros((10,1,28,28))`: This is our starting image. It's completely blank. We can make this whatever we want so long as it is between the clip values. Try starting with 1s, 0.5s, and random digits and see how that impacts results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121f82e-37e4-408f-9b28-de81870c6ac5",
   "metadata": {},
   "source": [
    "### SOLUTION: Exercise - \"When Inversion Doesn't Work\"\n",
    "You can maybe gather from the name of the section that this won't go quite as well. This exercise might be frustrating - the goal is not for you to get a perfect inversion. It's for you to understand the differences that can be made from tweaking attack setup, and how the model and training data can impact our ability to invert a model.\n",
    "\n",
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9c964-202f-43e5-8b6a-4804c6618a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model from the pytorch hub\n",
    "target_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', weights='MobileNet_V2_Weights.DEFAULT', verbose=False)\n",
    "\n",
    "# Put model in evaluation mode\n",
    "target_model.eval()\n",
    "\n",
    "# put the model on a GPU if available, otherwise CPU\n",
    "target_model.to(device);\n",
    "\n",
    "# Define the transforms for preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),  # Crop the image to 224x224 about the center\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Normalize the image with the ImageNet dataset mean values\n",
    "        std=[0.229, 0.224, 0.225]  # Normalize the image with the ImageNet dataset standard deviation values\n",
    "    )\n",
    "]);\n",
    "\n",
    "unnormalize = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "   std= [1/s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "with open(\"../data/labels.txt\", 'r') as f:\n",
    "    labels = [label.strip() for label in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a18415-09a9-405e-b0b7-8d3dd423ab63",
   "metadata": {},
   "source": [
    "We use a bubble target class because it is likely the easiest one for you to invert at least _some_ of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27422507-edb9-466c-9639-0df7e81842b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([971]) # bubble\n",
    "classifier = PyTorchClassifier(\n",
    "    model=target_model,\n",
    "    loss=F.cross_entropy,\n",
    "    input_shape=(1, 3, 128, 128),\n",
    "    nb_classes=1000,\n",
    "    clip_values=(0,1)\n",
    ")\n",
    "\n",
    "attack = MIFace(\n",
    "    classifier,\n",
    "    max_iter=5000,\n",
    "    learning_rate=0.5,\n",
    "    threshold=1.\n",
    ") \n",
    "\n",
    "\n",
    "x_train_infer = np.zeros((1, 3, 128, 128))\n",
    "x_adv = attack.infer(x=x_train_infer, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388014f-c01c-4a8d-a18e-27a5550ff029",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(1, 1),  # creates 2x2 grid of axes\n",
    "             axes_pad=0.3,\n",
    "                 )\n",
    "\n",
    "im = x_adv[0]\n",
    "grid[0].imshow(im.T)\n",
    "grid[0].set_title(labels[y[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4a71f-312e-4740-aae8-ec3bf75452de",
   "metadata": {},
   "source": [
    "Did you get a bubble? No? We wouldn't expect it to be an exact replica with such a small number of iterations. \n",
    "\n",
    "Additionally, compared to MNIST, this dataset has a very high degree of variability even within classes. While MNIST digit images are highly normalized, centered, and displayed against a consistent black background (making it relatively easy to invert an aggregate representation), ImageNet has extreme variability within classes. We wouldn't expect the model to hand over an image that perfectly resembles a target class, but with any luck, we can get some of the key features.\n",
    "\n",
    "Try the following:\n",
    "- Try different `max_iter` and compare the outputs (500, 800, 1000, 5000)\n",
    "- Try shrinking the `learning_rate` and compare the outputs (0.1, 0.25, etc)\n",
    "- Try different starting values for `x`:\n",
    "    - `np.random.randn(1, 3, 128, 128)`\n",
    "    - `np.ones((1, 3, 128, 128))`\n",
    "    - `np.ones((1, 3, 128, 128)) * 0.5`\n",
    "- Try manipulating the `threshold` argument to `MIFace` - setting it to `1.` will cause it to iterate through the full amount of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc853b-0545-415e-a0ae-a87659a8d64d",
   "metadata": {},
   "source": [
    "## SOLUTION: Exercise - Membership Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ced3d1-3803-4a60-a45f-58aa93d7e70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"compvis/stable-diffusion-v1-1\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae2ebe-5fae-443d-865b-3c4bcf2b547e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Ann Graham Lotz\" # from the paper :) \n",
    "\n",
    "i = 716907\n",
    "torch.manual_seed(i)\n",
    "imagesnp = pipe(prompt, num_images_per_prompt=25, output_type=\"np.array\").images\n",
    "images = pipe.numpy_to_pil(imagesnp)\n",
    "\n",
    "fig = plt.figure()\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(5, 5),  # creates 2x2 grid of axes\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, images):\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000c39f-6839-4108-80b0-7039bd30b8af",
   "metadata": {},
   "source": [
    "You're unlikely to get the actual image from the paper with this small of a number of retrieved images, but there should be some that do look very similar. The takeaway here should be that the model returns lots of images that look very similar to Ann Graham Lotz, which isn't great. \n",
    "\n",
    "### SOLUTION: Exercise - \n",
    "\n",
    "In the paper, they make the case that images returned by the model that are very similar are more likely to be memorized. Adjust the above display to only show images that are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bec51e-0391-4c9a-9838-bf4733a0a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn = torch.nn.MSELoss()\n",
    "imagest = torch.Tensor(imagesnp)\n",
    "fig = plt.figure()\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(5, 5),  # creates 2x2 grid of axes\n",
    "                 )\n",
    "for i in range(len(imagest)):\n",
    "    for j in range(i,len(imagest)):\n",
    "        if lossfn(imagest[i],imagest[j]) < 0.1: # tune to something that works\n",
    "            grid[i].imshow(images[i])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e381dd-2087-4195-bb87-1f718c6fe071",
   "metadata": {},
   "source": [
    "## SOLUTION: Exercise - Gray Sky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0555283-bac8-4796-847b-69e871c4e63a",
   "metadata": {},
   "source": [
    "You need to read a bit of the paper for this one, specifically section 4.2.1. There are many possible ways to go about this.\n",
    "\n",
    "The first chunk you need is:\n",
    "\n",
    "> We instead design a new membership inference attack strategy based\n",
    "on the intuition that for diffusion models, with high probability Gen(p;r1) 6= Gen(p;r2) for two different random\n",
    "initial seeds r1,r2. On the other hand, if Gen(p;r1) ≈d\n",
    "Gen(p;r2) under some distance measure d, it is likely\n",
    "that these generated samples are memorized examples.\n",
    "\n",
    "So let's generate a handful of images for two different random seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804d86c-5901-46f4-8ad8-29ca8f24069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A gray sky\"\n",
    "seeds = [716907, 123456]\n",
    "res_images = {}\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    imagesnp = pipe(prompt, num_images_per_prompt=12, output_type=\"np.array\").images\n",
    "    images = pipe.numpy_to_pil(imagesnp)\n",
    "    imagest = torch.Tensor(imagesnp)\n",
    "    res_images[seed] = imagest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989d86d-c23e-48cf-badf-59b1fff9ea07",
   "metadata": {},
   "source": [
    "The next chunk is here:\n",
    "\n",
    "> To compute the distance measure d among the images\n",
    "in the clique, we use a modified Euclidean L2 distance.\n",
    "In particular, we found that many generations were often\n",
    "spuriously similar according to L2 distance (e.g., they all\n",
    "had gray background). We therefore instead divide each\n",
    "image into 16 non-overlapping 128×128 tiles and measure the maximum of the L2 distance between any pair of\n",
    "image tiles between the two images.\n",
    "\n",
    "\n",
    "First, we'll define a function to break the images into tiles and then compute the L2 distance between all respective tiles in the two images. We'll return the max L2 between the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5d7c4-e43e-499e-a3c2-3c83c4e19cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_image_distance(img1, img2):\n",
    "    \"\"\"\n",
    "    Compute custom distance between two image tensors.\n",
    "    \n",
    "    :param img1: PyTorch tensor of shape (512, 512, 3)\n",
    "    :param img2: PyTorch tensor of shape (512, 512, 3)\n",
    "    :return: Maximum L2 distance between any pair of 128x128 tiles\n",
    "    \"\"\"\n",
    "    assert img1.shape == img2.shape == (512, 512, 3), \"Images must be 512x512x3\"\n",
    "    \n",
    "    # Reshape images into 16 tiles of 128x128x3\n",
    "    img1_tiles = img1.reshape(4, 128, 4, 128, 3).permute(0, 2, 1, 3, 4).reshape(16, 128*128*3)\n",
    "    img2_tiles = img2.reshape(4, 128, 4, 128, 3).permute(0, 2, 1, 3, 4).reshape(16, 128*128*3)\n",
    "    \n",
    "    # Compute pairwise L2 distances between all tiles\n",
    "    distances = torch.cdist(img1_tiles, img2_tiles, p=2)\n",
    "    \n",
    "    # Return the maximum distance\n",
    "    return distances.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b163cc-6593-4d9f-a81b-007d3a37aeff",
   "metadata": {},
   "source": [
    "Next, we'll define a function that will generate a 12 x 12 matrix (we generated 12 images per seed) so we can see the distance between each pair of images using our new distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f6be4-39c0-44a5-a1e2-0a17d6228435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(tensor_list):\n",
    "    \"\"\"\n",
    "    Compute distance matrix for a list of image tensors.\n",
    "\n",
    "    Requires len(tensor_list) == 2\n",
    "    \n",
    "    :param tensor_list: List of 2 lists of PyTorch tensors, each of shape (512, 512, 3)\n",
    "    :return: Distance matrix as a PyTorch tensor between the two provided lists\n",
    "    \"\"\"\n",
    "    distance_matrix = torch.zeros((len(tensor_list[0]), len(tensor_list[1])))\n",
    "\n",
    "    for i in range(len(tensor_list[0])):\n",
    "        for j in range(len(tensor_list[1])):\n",
    "            im1, im2 = tensor_list[0][i], tensor_list[1][j]\n",
    "            dist = custom_image_distance(im1, im2)\n",
    "            distance_matrix[i,j] = dist\n",
    "            #distance_matrix[j,i] = dist # because symmetry\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac41a49-aad6-44d4-a174-e769acc69704",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = compute_distance_matrix([res_images[i] for i in seeds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6704dd-84fe-42d1-b481-408bf6270416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then visualize this matrix as before:\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(distance_matrix.numpy(), annot=True, cmap='YlGnBu')\n",
    "plt.title('Custom Image Distance Matrix')\n",
    "plt.xlabel(f'Image Index for List from seed {seeds[1]}')\n",
    "plt.ylabel(f'Image Index for List from seed {seeds[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f855f3-c70f-40ce-8a72-933a6c6e91d9",
   "metadata": {},
   "source": [
    "From here we see there are a handful of pairs that appear pretty close relative to others. Let's visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd1edd-7176-4615-ba28-7308e68747d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1, im2 = res_images[716907][3], res_images[123456][0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(im1)\n",
    "ax2.imshow(im2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf609e73-2a06-4de9-8a8b-09dda8f1ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1, im2 = res_images[716907][1], res_images[123456][7]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(im1)\n",
    "ax2.imshow(im2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976e882-6e3c-42b5-af04-7a3dc041158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1, im2 = res_images[716907][3], res_images[123456][6]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(im1)\n",
    "ax2.imshow(im2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ea749-edc4-4a86-bb7d-959d4a84fd2a",
   "metadata": {},
   "source": [
    "They're not perfect, but it does start to narrow in on images that have very similar qualities. Try some other seeds and play around with this. Read more of the paper to better understand why identifying similar images across random seeds may indicate memorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0eddd-63f2-4a57-9b62-eb91fd82c18e",
   "metadata": {},
   "source": [
    "# More Practice\n",
    "You can try your hand at more of these on [Crucible](https://crucible.dreadnode.io/challenges/inversion), and AI CTF platform. There's even an inversion challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8fb06-1767-4dfe-914a-c328e1b8c7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
