{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Closed-Box Evasion Answer Key\n",
    "This answer key is configured such that you should be able to run the code here and see possible approaches to a working solution. For each topic, it will also link further resources, and go into more detail on certain code chunks. It is not meant to be edited. \n",
    "\n",
    "Use these answer keys as a guide as needed. Try to work use the context here to work toward an answer before reaching for the solution.\n",
    "\n",
    "**If you just want to see the answers, they're all tagged with \"SOLUTION\", CTRL+F your heart out.**\n",
    "\n",
    "## Setup\n",
    "The setup code must be run for the solutions to work properly. Review the breakdown of the setup code in the lab notebook for an explanation of each section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#load the model from the pytorch hub\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', weights='MobileNet_V2_Weights.DEFAULT', verbose=False)\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# put the model on a GPU if available, otherwise CPU\n",
    "model.to(device);\n",
    "\n",
    "# Define the transforms for preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),  # Crop the image to 224x224 about the center\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Normalize the image with the ImageNet dataset mean values\n",
    "        std=[0.229, 0.224, 0.225]  # Normalize the image with the ImageNet dataset standard deviation values\n",
    "    )\n",
    "]);\n",
    "\n",
    "unnormalize = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "   std= [1/s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "with open(\"../data/labels.txt\", 'r') as f:\n",
    "    labels = [label.strip() for label in f.readlines()]\n",
    "\n",
    "img = Image.open(\"../data/dog.jpg\")\n",
    "img_tensor = preprocess(img).unsqueeze(0)\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "\n",
    "print(f\"Image tensor on device:\\n---------------\\n{img_tensor.device}\\n\")\n",
    "print(f\"Inputs information:\\n---------------\\nshape:{img_tensor.shape}\\nclass: {type(img_tensor)}\\n\")\n",
    "print(f\"Shape of outputs:\\n---------------\\n{output.shape}\\n\")\n",
    "print(f\"Pred Index:\\n---------------\\n{output[0].argmax()}\\n\")\n",
    "print(f\"Pred Label:\\n---------------\\n{labels[output[0].argmax()]}\\n\")\n",
    "\n",
    "unnormed_img_tensor= unnormalize(img_tensor)\n",
    "\n",
    "img_pil = transforms.functional.to_pil_image(unnormed_img_tensor[0])\n",
    "img_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimBA (Simple BlackBox Attack)\n",
    "### Resources & Setup\n",
    "- [SimBA Paper](https://arxiv.org/abs/1905.07121)\n",
    "- [Attack implementation code](https://github.com/cg563/simple-blackbox-attack)\n",
    "\n",
    "Start by reloading the image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"../data/dog.jpg\")\n",
    "img_tensor = preprocess(img).unsqueeze(0)\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "\n",
    "print(f\"Image tensor on device:\\n---------------\\n{img_tensor.device}\\n\")\n",
    "print(f\"Inputs information:\\n---------------\\nshape:{img_tensor.shape}\\nclass: {type(img_tensor)}\\n\")\n",
    "print(f\"Shape of outputs:\\n---------------\\n{output.shape}\\n\")\n",
    "print(f\"Pred Index:\\n---------------\\n{output[0].argmax()}\\n\")\n",
    "print(f\"Pred Label:\\n---------------\\n{labels[output[0].argmax()]}\\n\")\n",
    "\n",
    "unnormed_img_tensor= unnormalize(img_tensor)\n",
    "\n",
    "img_pil = transforms.functional.to_pil_image(unnormed_img_tensor[0])\n",
    "img_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack\n",
    "### Provided Code\n",
    "The code below is provided in the lab and must be run for the exercise solution to work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_masks = 1000\n",
    "eta = 0.005\n",
    "\n",
    "# Generate a tensor that is a collection of \"masks\"\n",
    "# The tensor will have 1000 copies of tensors with the same shape as img_tensor\n",
    "# The values will have a mean 0 and variance 1 and be scaled down by eta \n",
    "mask_collection = torch.randn((n_masks, *img_tensor.shape)).to(device) * eta\n",
    "\n",
    "# initial mask with shape of img_tensor and values of 0\n",
    "current_mask = torch.zeros_like(img_tensor).to(device)\n",
    "\n",
    "# compute our starting index\n",
    "starting_index = model(img_tensor).argmax(1)\n",
    "print(f\"Starting index is:\\n---------------\\n{starting_index}\\n\")\n",
    "\n",
    "starting_class_score = model(img_tensor + current_mask)[0, starting_index.item()].item()\n",
    "print(f\"Starting class score is:\\n---------------\\n{starting_class_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The untargeted attack is given in the lab. \n",
    "\n",
    "### SOLUTION: Exercise 1\n",
    "We need to have the image of the dog be classified as a robin. The intuition here is slightly different than the untargeted. We want to only apply masks from our mask candidates that will improve the score of the _target_ class returned in the logits from the model.\n",
    "\n",
    "This intuition can be visualized here.\n",
    "\n",
    "![](../assets/2-evasion-simba.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Zero our current mask\n",
    "current_mask = torch.zeros_like(img_tensor).to(device)\n",
    "\n",
    "# Get our starting label index\n",
    "starting_label = model(img_tensor).argmax(1).item()\n",
    "current_label = starting_label\n",
    "\n",
    "# Target class index\n",
    "target_index = torch.tensor(labels.index('robin')).unsqueeze(0).to(device)\n",
    "\n",
    "# Get our starting confidence score\n",
    "best_score = model(img_tensor + current_mask)[0, target_index.item()].item()\n",
    "\n",
    "# Run until we reclassify successfully ...\n",
    "while current_label != target_index:\n",
    "\n",
    "    # Select a random mask from the collection we created\n",
    "    mask_candidate_idx = np.random.choice(len(mask_collection))\n",
    "    mask_candidate = mask_collection[mask_candidate_idx]\n",
    "\n",
    "    # Don't store gradient information while doing inference\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor + current_mask + mask_candidate)\n",
    "    \n",
    "    # Based on our mask addition, get our new label and updated score\n",
    "    current_label = output.argmax(1).item()\n",
    "    new_score = output[0, target_index.item()].item()\n",
    "\n",
    "    # If we haven't hit our target yet and also didn't improve the score, just move on\n",
    "    # NOTE CHANGED TARGET_LABEL TO STARTING_LABEL because our goal is to NOT BE what we are more than be a specific thing\n",
    "    if new_score < best_score:\n",
    "        continue\n",
    "\n",
    "    # Write some monitoring for dopamine \n",
    "    print(f\"Best score is: {best_score:4.6f} -- pred score is: {output[0, current_label].item()} -- prediction is: {current_label}  \", end='\\r', flush=True)\n",
    "    \n",
    "    # Update our current score\n",
    "    best_score = new_score\n",
    "    \n",
    "    # And update our mask\n",
    "    current_mask += mask_candidate\n",
    "                \n",
    "print(f\"\\n\\nWinner winner: {labels[output[0].argmax()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Help me understand_\n",
    "- **What changed between these approaches?** We are now moving toward a target class, not away from the original. With that, we only care about the score that the model returns for the _target_ class. `new_score = output[0, target_index.item()].item()` grabs the logit the model returned for the target index. We then skip any candidate masks that do not provide a score as good or better than the best score.\n",
    "- **Why do we randomly sample the candidate masks instead of interating through them?** We are _accumulating_ a final mask. Remember when we generated our large random mask tensor, we dramatically downscaled the mask individual values. Here we are slowly building a mask out of those tiny random perturbations. `current_mask += mask_candidate` adds the `mask_candidate` that, when added to the `current_mask`, improved our classification score for the target class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HopSkipJump\n",
    "### Resources\n",
    "All are optional. The video at the top of the lab should provide you with the basic context, this is just if you want to know more.\n",
    "- [HopSkipJump Paper](https://arxiv.org/abs/1904.02144)\n",
    "- [\"I ain't reading all that\", fine, here's a video someone made on the paper](https://www.youtube.com/watch?v=vkCifg2rp34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/2-evasion-hsj.png)\n",
    "\n",
    "*Source: linked paper above*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No code here, these exercises are really just about your understanding of the intuition behind HopSkipJump.\n",
    "\n",
    "### SOLUTION: Exercise 2\n",
    "\"Why is the `normalized_gradient` the same shape and size as our `img_tensor`\"?\n",
    "\n",
    "A gradient, at its core, is all of a function's partial derivatives. Everything comes back to calculus. \n",
    "\n",
    "Say we have a function \n",
    "\n",
    "$$\n",
    "f(x, y) = x^3 + 3y^2\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has **two variables** and therefore **two partial derivatives**. Remember when we take the partial derivative of a function with respect to one variable, we treat all other variables as constants. Using the visual above, you can think of the partial derivative of the function with respect to x at any point \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{(x^3 + 3y^2)}}{\\partial{x}} = 3x^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{(x^3 + 3y^2)}}{\\partial{y}} = 6y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of a tensor the partial derivative of a function (a loss function in our case) with respect to each individual element. So the gradient of $f(x, y) = x^3 + 3y^2$ would just be a 2 element vector, $[3x^2, 6y]$. \n",
    "\n",
    "The gradient of a function will always have the same number of components as the number of variables in the function. The gradient is a vector of partial derivatives of the loss function with respect to each input element. It shows how the loss changes with small changes in each input dimension. It just so happens in our case that _every_ pixel in the image is a \"variable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION: Exercise 3\n",
    "\n",
    "Okay, I lied, you will need to run this chunk of setup code for these solutions to work in this notebook. This way you don't need to jump back and forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "# move sample to the right device\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "    \n",
    "y_original = output[0].argmax()\n",
    "\n",
    "def adversarial_satisfactory(samples, target, clip_min, clip_max):\n",
    "    samples = torch.clamp(samples, clip_min, clip_max)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = model(samples).argmax(dim=1)\n",
    "    \n",
    "    # any other class\n",
    "    result = preds != target\n",
    "    return result\n",
    "\n",
    "clip_min = -2\n",
    "clip_max = +2\n",
    "\n",
    "# randomly seed the generator\n",
    "generator = torch.Generator().manual_seed(0)\n",
    "\n",
    "# now generate a misclassified sample; we'll give ourselves 10 tries\n",
    "for _ in range(10):\n",
    "    random_img = torch.FloatTensor(img_tensor.shape).uniform_(clip_min,clip_max, generator=generator).to(device)\n",
    "    random_class = model(random_img).argmax()\n",
    "\n",
    "    if adversarial_satisfactory(random_img, y_original, clip_min, clip_max):\n",
    "        initial_sample = (random_img, y_original)\n",
    "\n",
    "        print(f\"Found misclassified image: {random_class}\")\n",
    "        break\n",
    "\n",
    "threshold = 0.01 / torch.sqrt(torch.prod(torch.tensor(img_tensor.shape, dtype=torch.float).to(img_tensor.device)))\n",
    "\n",
    "upper_bound, lower_bound = 1,0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The boundary adversarial example does in fact get closer to the original image in img_tensor every time we project then bisect. (think distance!)\n",
    "\n",
    "We can iteratively repeat the projection and bisection happening within the `while` loop. It currently stops as soon as the adversarial conditions are satisfied, but we could move it closer and closer by simply repeating the inner steps in that loop for N iterations, and add the computation of the L2 norm between the current anchor image and the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for i in range(100):\n",
    "    boundary_adversarial_example = (1-upper_bound)*img_tensor + upper_bound*random_img\n",
    "    distance = torch.norm(boundary_adversarial_example - img_tensor, p=2)\n",
    "    distances.append(distance.item())\n",
    "    midpoint = (upper_bound + lower_bound) / 2.0\n",
    "    \n",
    "    interpolated_sample = (1 - midpoint) * img_tensor + midpoint * random_img\n",
    "    \n",
    "    if adversarial_satisfactory(interpolated_sample, y_original, clip_min, clip_max):\n",
    "        # the decision boundary lies between midpoint and lower\n",
    "        upper_bound, lower_bound = midpoint, lower_bound\n",
    "    else:\n",
    "        # it's the other way\n",
    "        upper_bound, lower_bound = upper_bound, midpoint\n",
    "    \n",
    "boundary_adversarial_example = (1-upper_bound)*img_tensor + upper_bound*random_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The distance between the original image and the modified image begins to slowly stabilize to some constant value. (track distance through time)\n",
    "\n",
    "If we run the above code, you'll see it converges around 140 very quickly. We definitely don't need 100 iterations. We can see this via a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(distances)), distances, marker='o')\n",
    "\n",
    "plt.title('Distance between Anchor and Original w.r.t. HSJ Iteration')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (Optional) Add an early exit criteria to the HopSkipJump attack to reduce the number of model calls (don't just reduce the number of iterations; stop in a data-driven way), and compare the results from the full run to the early exit run. As an attacker, why might you want to early exit from the optimization?\n",
    "\n",
    "In the same way as above, we can use the distance between the new anchor image and the target image. As we see, it converges pretty quickly. We could add an early exit condition to halt when the distance stops changing by some treshold amount. \n",
    "\n",
    "Why? Well, we're an attacker, and we don't want to get caught! Hammering a model like this could trigger detections or rate limits if we aren't careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Practice\n",
    "You can try your hand at more of these at [Crucible](crucible.dreadnode.io), an AI CTF. The \"Granny\" challenge in particular may interest you after this lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
