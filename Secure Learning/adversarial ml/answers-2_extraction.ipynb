{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6366c2-0e83-49ae-b2bb-826b62d60344",
   "metadata": {},
   "source": [
    "# Extraction the Hard Way Answer Key\n",
    "This lab is a bit more open ended then the rest, so the exercises don't all have exact solutions. Instead, we'll provide a bit more context around what you should be looking for in the EDA, how to get started exploring certain parts of the data, and a bit more guidance on how to use what you discover.\n",
    "\n",
    "With that, we've left in a number of the inline solutions in the lab and instead will use this space as a means to get you unstuck if you find yourself stuck.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "We have a lot of \"scores\". We don't know how they're being generated or how they interact. In theory, we see an `mlx` score and you should see in the initial `.describe()` call that all of the emails marked as `spam` have `reason` as `mlx`. In other words, it's some kind of ML score that is being generated using the email data. We want to analyze this and the other scores to try and understand how they might be useful for identifying potential phishing emails. We also want to learn more about how the scores are being generated from the email content itself.\n",
    "\n",
    "You're told to first identify interesting columns. I'd start with all the score columns.\n",
    "\n",
    "```python\n",
    "score_columns = ['score', 'bulkscore', 'priorityscore', 'spamscore', 'mlxscore', \n",
    "                 'mlxlogscore', 'lowpriorityscore', 'suspectscore', 'adultscore', 'clxscore']\n",
    "```\n",
    "\n",
    "You can also get crafty and engineer some features, like the length of the content.\n",
    "\n",
    "### Inferring Decision Boundaries\n",
    "You're told to make more charts of things you think would be cool to look at. There isn't a solution here because... do what you want! \n",
    "\n",
    "Some places to start... \n",
    "\n",
    "#### Correlation Heatmap\n",
    "How do our scores correlate?\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "score_columns = ['score', 'bulkscore', 'priorityscore', 'spamscore', 'mlxscore', \n",
    "                 'mlxlogscore', 'lowpriorityscore', 'suspectscore', 'adultscore', 'clxscore']\n",
    "\n",
    "score_df = df[score_columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = score_df.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Heatmap of Score Columns')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Do you see anything that seems to directly correlate with the `mlx` score(s)?\n",
    "\n",
    "#### Pairplot\n",
    "A pairplot can be used to make scatterplots of numerical values to give us a more in-depth look at how the fields might relate to each other.\n",
    "\n",
    "```python\n",
    "sns.pairplot(score_df)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Look closely - do you notice any fields that relate to one another perhaps without being immedaitely correlated? What can this tell us about how `mlxscore` is computed?\n",
    "\n",
    "#### Content Features vs Score\n",
    "You can also get crafty and make some plots to understand how the content itself relates to the `logmlxscore` (or any other scores). Here we'll look at the overall wordcount, the mean word length, and the proportion of the content made up of non-alphanumeric characters. Go nuts!\n",
    "\n",
    "```python\n",
    "# Function to count words\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Function to calculate mean word length\n",
    "def mean_word_length(text):\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "# Function to calculate proportion of non-alphanumeric characters\n",
    "def prop_non_alphanum(text):\n",
    "    total_chars = len(text)\n",
    "    non_alphanum = sum(not c.isalnum() for c in text)\n",
    "    return non_alphanum / total_chars if total_chars > 0 else 0\n",
    "\n",
    "# Calculate new columns\n",
    "df['word_count'] = df['content'].apply(word_count)\n",
    "df['avg_word_length'] = df['content'].apply(mean_word_length)\n",
    "df['prop_non_alphanum'] = df['content'].apply(prop_non_alphanum)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Number of words vs logmlxscore\n",
    "sns.scatterplot(x='word_count', y='mlxlogscore', data=df, ax=ax1)\n",
    "ax1.set_title('Number of Words vs Log MLX Score')\n",
    "ax1.set_xlabel('Number of Words')\n",
    "ax1.set_ylabel('Log MLX Score')\n",
    "\n",
    "# Plot 2: Mean word length vs logmlxscore\n",
    "sns.scatterplot(x='avg_word_length', y='mlxlogscore', data=df, ax=ax2)\n",
    "ax2.set_title('Mean Word Length vs Log MLX Score')\n",
    "ax2.set_xlabel('Mean Word Length')\n",
    "ax2.set_ylabel('Log MLX Score')\n",
    "\n",
    "# Plot 3: Proportion of non-alphanumeric characters vs logmlxscore\n",
    "sns.scatterplot(x='prop_non_alphanum', y='mlxlogscore', data=df, ax=ax3)\n",
    "ax3.set_title('Proportion of Non-Alphanumeric Characters vs Log MLX Score')\n",
    "ax3.set_xlabel('Proportion of Non-Alphanumeric Characters')\n",
    "ax3.set_ylabel('Log MLX Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Dimensionality Reduction\n",
    "We started by vectorizing our email content. Our `word_doc_matrix` will have the same number of rows as the email dataset, and the columns will be equal to the number of unique terms in the vocabulary constructed from all of the email content. A \"term\" here is referred to as an \"n-gram\" which is either one or two words. \n",
    "### Resources\n",
    "- What is PCA (Principal Component Analysis) actually?\n",
    "    - PCA is a method of dimensionality reduction. Right now our `word_doc_matrix` has a lot of columns. We need a way of turning our many columns (dimensions) into dimensions that can take into account all of the data and map it to a N dimensional space. PCA does this by finding the top N eigenvectors of the covariance matrix that have the largest eigenvalues. This gives us the N **principal components**. They essentially tell us the axes upon which the **data varies the most**. When we multiply the data by these N eigenvectors, the data will be \"mapped\" onto an N-dimensional space. In this case 2.\n",
    "    - \"Those are words I haven't heard in a long time\" if you really want to understand it beyond \"it does some math to find new axes that better separate the data\" then I recommend [StatQuest](https://www.youtube.com/watch?v=HMOI_lkzW08).\n",
    "- What is t-SNE and why is it different?\n",
    "    - For the purposes here, it is more dimensionality reduction, but unlike PCA it tries to preserve similarities in the new dimensional mapping. Points that were \"close\" in the original Z-dimensional space should remain \"close\" in the new N-dimensional space.\n",
    "    - The **perplexity** value impacts whether the algorithm focuses more on maintaining local structure between smaller numbers of data points (smaller value of perplexity) vs. optimizing for maintaining the global structure of the data (larger perplexity). Too small and you might see disjoint fragmented clusters. Too large and you might just get a big blob. Play with it!\n",
    "\n",
    "\n",
    "When we perform the reduction, depending on your setup, you might see some nice clusters appear! These tell us that there might be some decision boundaries that exist in this new dimensionality that allows us to group together similarly spammy messages.\n",
    "\n",
    "![](../assets/2_extraction_tsne.png)\n",
    "\n",
    "## Testing out a Model\n",
    "In the lab, you've loaded a sequence classification model that will learn from labeled text and be able to classify our text samples. We first selected a threshold for the `mlxscore` and labeled emails based on whether their score was above or below the threshold.\n",
    "\n",
    "Then you train a model using those labels, and try to write phishing email samples that will successfully subvert detection, e.g. yield a \"sub-threshold\" classification.\n",
    "\n",
    "Can't land on a good threshold? Try some empirical methods. Write email text that you _know_ should be classified as spam. Start your threshold high, then bring it down until you land on a threshold that will work at classifying your phishy email as spam. \n",
    "\n",
    "Then, leverage the techniques above to tweak your email and subvert detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
