{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649d23d5",
   "metadata": {},
   "source": [
    "![DLI Logo](../assets/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb4bee",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this module, you will learn how to think about machine learning problems and attacks as **optimizations** and we'll introduce a new tool: [Optuna](https://optuna.org/).\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Apply `Optuna` to optimize attack hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69fded",
   "metadata": {},
   "source": [
    "# Optimize\n",
    "We've reached a crucial milestone and we're about to come full circle on a whole load of concepts. We ended the previous section lamenting that there are entirely too many techniques to know which is best. Even if you knew which algorithm was best for your situation, there are _still_ hyperparameters to choose. We've conveniently hand-waved explaining hyperparameters in any real detail in anticipation of this lab. Rather than optimize a model (for which there are several references), we're going to optimize our attacks using Optuna.\n",
    "\n",
    "Optuna is an open-source hyperparameter optimization (HPO) framework, which is designed to optimize machine learning model parameters. It's normally used to automate the process of finding the best set of hyperparameters for a model. Recall that hypterparameters are typically the \"fixed\" values of an algorithm that define behavior or constraints (like a distance metric) it needs to work within. Optuna basically builds another model using Bayesian optimization techniques to infer the optimal values.\n",
    "\n",
    "1. **Define a Prior**: This is an initial assumption about the function. \n",
    "2. **Collect Data and Update the Prior**: This involves evaluating the actual function at certain points, and then using this data to update our prior belief about the function. This updated belief is called the posterior, and it represents a kind of best guess at what the function looks like, given the current data.\n",
    "3. **Choose the Next Point to Evaluate**: After updating the posterior, we need to decide where to evaluate the function next. This is done by applying an acquisition function to the posterior. The acquisition function trades off exploration (testing areas where we are uncertain about the function) and exploitation (focusing on areas where the function seems high). Common choices for the acquisition function include `Expected Improvement`, `Probability of Improvement`, and `Upper Confidence Bound`.\n",
    "4. **Iterate**: Steps 2 and 3 are then repeated.\n",
    "\n",
    "The goal of using Optuna (and HPO in general) is to find the set of hyperparameters that will result in the best performance for a given machine learning model, based on a specified evaluation metric. This process involves defining a space of possible hyperparameters and then systematically exploring this space, typically through multiple `trials`. An advantage of Bayesian Optimization is that it doesn't require any derivatives (gradients) of the function, which makes it suitable for optimizing \"Blackbox\" functions. It's perhaps best to think of Optuna as an optimizer of processes, rather than data. \n",
    "\n",
    "In this lab we're going to revisit work from previous labs and apply optimization techniques to make them _better_. \n",
    "\n",
    "\n",
    "# Imports and Model\n",
    "We'll start by importing everything we need at the top and loading our target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054426c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from libs.controls import modifier\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from art.estimators.classification import BlackBoxClassifierNeuralNetwork\n",
    "from art.utils import compute_success\n",
    "from art.attacks.evasion import SimBA\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b691935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "target_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', weights='MobileNet_V2_Weights.DEFAULT', verbose=False)\n",
    "target_model.eval()\n",
    "target_model.to(device);\n",
    "\n",
    "# Define the transforms for preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "]);\n",
    "\n",
    "unnormalize = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "   std= [1/s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "with open(\"../data/labels.txt\", 'r') as f:\n",
    "    labels = [label.strip() for label in f.readlines()]\n",
    "\n",
    "img = Image.open(\"../data/dog.jpg\")\n",
    "img_tensor = preprocess(img).unsqueeze(0)\n",
    "unnormed_img_tensor = unnormalize(img_tensor).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ea198",
   "metadata": {},
   "source": [
    "# Optuna\n",
    "Optuna is easy to use, here is how we setup an optimization problem. We'll use a toy example where we want to find the minimum of the function $(x-2)^2$.  The true answer is $x=2$, but Optuna doesn't know that and will need to sample the space we define and and infer the answer from the examples it creates.\n",
    "\n",
    "First we define an `objective` function.  Think of this like a loss function. Here we want to minimize the squared difference between $x$ and $2$.  We'll define the sample space as the float range $-10<x<10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float('x', -10, 10)\n",
    "    return (x - 2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f77ef3",
   "metadata": {},
   "source": [
    "Then we create a \"study\". `create_study` provides an entry point to create and configure a `Study` instance, which then gets passed to `optimize` (and other functions to carry out the tuning process). When you call `optimize` on the `Study`, the `Study` manages the full optimization loop internally, leveraging the Study's samplers and algorithms to effectively search the hyperparameter space. Then, after `n_trials` have been completed, Optuna will have the \"best\" parameters from within the tested range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "study = optuna.create_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8615b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Run the objective n_trials times to optimize\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9826168-5a7d-43c1-867c-f3059c801b5d",
   "metadata": {},
   "source": [
    "Let's see how well Optuna did finding the minimum.  You can run the cell above with more `n_trials` to get closer to the true minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64debf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "print(f\"Best params::\\n---------------\\n{study.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b5d6f",
   "metadata": {},
   "source": [
    "By default, Optuna uses Tree-Parzen Estimator (TPE), a sequential model-based optimization (SMBO) approach that builds a probabilistic model of the objective function to suggest new parameters. TPE specifically models $P(x|y)$ and $P(y)$ where $x$ represents parameters and $y$ is the associated cost. The Bayesian base of this algorithm means it becomes better at selecting parameters as the number of trials increase. \n",
    "\n",
    "- **Initialization**: Start with initial random samples from the defined space and their corresponding objective function values.\n",
    "- **Model building**: Using the collected data, build two probability models, `l(x)` for parameters that improved the model's performance and `g(x)` for parameters that did not improve. The model used to estimate these probabilities.\n",
    "- **Suggestion**: For the next round, suggest new hyperparameters to try. This is based on calculating the Expected Improvement (EI) over the current best parameters, where the EI of a set of hyperparameters is proportional to the ratio `l(x) / g(x)`. This suggests that we prefer regions of the space where good hyperparameters are more likely than bad ones according to the model.\n",
    "- **Iteration**: Evaluate the objective function with the new parameters, update the models, and repeat from step 2. Over time, the TPE algorithm should hone in on the best parameters.\n",
    "\n",
    "The primary of advantages of TPE (and other SMBO techniques) is that it can handle both continuous and discrete hyperparameters, and it doesn't require the objective function to be differentiable (as gradient-based methods do)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10069850",
   "metadata": {},
   "source": [
    "# Optimize an Attack\n",
    "First, we'll bring forward ART and write a `predict` function for the attack to use. Then we build the attack as before. One thing you may notice is instead of `BlackBoxClassifier`, we now have `BlackBoxClassifierNeuralNetwork`. This is because we're using `SimBA` which requires probabilities and ART is reasonably strict about what is need upfront before you can start running an attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "class ModelWrapper: \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        torch_tensor = torch.from_numpy(x).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = target_model(torch_tensor)        \n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "\n",
    "model_wrapper = ModelWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "classifier = BlackBoxClassifierNeuralNetwork(\n",
    "    predict_fn = lambda x:model_wrapper.predict(x),\n",
    "    nb_classes = len(labels),\n",
    "    input_shape = img_tensor[0].shape\n",
    ")\n",
    "\n",
    "attack = SimBA(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "print(\"Attack params\\n---------------\")\n",
    "[print(f\"{i}: {attack.__dict__.get(i)}\") for i in attack.attack_params];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b8c9e-153a-408f-829c-dbd36eed18fd",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97677b",
   "metadata": {},
   "source": [
    "This is the part we've failed to mention - don't be mad. `SimBA` (and pretty much all attacks) have default hyperparameters that are set when the attack is created. However, these values are default, and we're not confident where they came from or the assumptions made when these values were chosen. We've learned that manually coming up the reasonable values is futile. So here we are, using yet another optimization technique to make a number go arbitrarily up or down. If you've managed to run into a certain `batch_size` and never figured it out; here is the answer to why `1` sample went to `64` samples in your `predict` functions in ART. \n",
    "\n",
    "Let's wrap our attack with Optuna and define what we want to minimize in our attack. In this case, we want to minimize `l2_norm`.  \n",
    "\n",
    "Here we write an objective function that runs a chosen `attack` (SimBA). As we're not giving Optuna any variables or suggested ranges, it won't optimize anything yet. \n",
    "\n",
    "You will see all the parameter values it chose (just an empty set for now) and the result of the `trial`, before printing the best parameters (none yet) at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def objective(trial, attack, x):\n",
    "    with torch.no_grad():\n",
    "        results = attack.generate(x=x)\n",
    "        \n",
    "    l2_norm = torch.norm(img_tensor - results, p=2).float()\n",
    "\n",
    "    return l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c46edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, attack=attack, x=img_tensor.numpy()), \n",
    "    n_trials=5, show_progress_bar=True\n",
    ");\n",
    "\n",
    "print(f\"\\nBest params::\\n---------------\\n{study.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61d7a8",
   "metadata": {},
   "source": [
    "The same result every time - this is a good baseline to start experimenting with. Now you have mechanics down, lets minimize this `L2` distance metric. Here we update the `objective` function to\n",
    "\n",
    "1. Give Optuna access to the attack parameters. Ranges for the paramaters here are arbitrarily chosen to bracket the default attack hyperparameters we exposed earlier.\n",
    "2. Update the attack with the new parameters \n",
    "3. Execute the attack\n",
    "4. Return the L2 distance between the original image and the adversarial image. This is what we want to minimize (arbitrarily)\n",
    "\n",
    "We hope to see is the `L2` distance going down..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def objective(trial, attack, x):\n",
    "    new_params = {\n",
    "        \"max_iter\": trial.suggest_int('max_iter',  10, 3000),\n",
    "        \"epsilon\": trial.suggest_float('epsilon', 1e-6, 1.0),\n",
    "        \"freq_dim\": trial.suggest_int('freq_dim', 1, 20),\n",
    "        \"stride\": 1,\n",
    "        \"batch_size\": 1,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    attack.set_params(**new_params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = attack.generate(x = x)\n",
    "        \n",
    "    l2_norm = torch.norm(img_tensor - results, p=2).float()\n",
    "\n",
    "    return l2_norm\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, attack=attack, x=img_tensor.numpy()), n_trials=50, show_progress_bar=True\n",
    ");\n",
    "\n",
    "print(f\"\\nBest params::\\n---------------\\n{study.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810357e2",
   "metadata": {},
   "source": [
    "That's quite the improvement over the default values for our `L2` metric! This metric is a little arbitrary as the attack technically already does this. Let's do something more useful. \n",
    "\n",
    "## Optimize 2 Values\n",
    "\n",
    "Here we add another metric that is more realistic - the number of queries, `num_queries`, we send the model. First we need to add logging functionality to our attack, which we will do in the predict function. Then we can simply return another value from the `objective` function and update the `study.optimize` call to provide a list of `directions`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94234769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "class ModelWrapper: \n",
    "    def __init__(self):\n",
    "        self.__reset__()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        torch_tensor = torch.from_numpy(x).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = target_model(torch_tensor)        \n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()\n",
    "        \n",
    "        self.num_queries += 1\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def __reset__(self):\n",
    "        self.num_queries = 0\n",
    "\n",
    "model_wrapper = ModelWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "classifier = BlackBoxClassifierNeuralNetwork(\n",
    "    predict_fn = lambda x:model_wrapper.predict(x),\n",
    "    nb_classes = len(labels),\n",
    "    input_shape = img_tensor[0].shape\n",
    ")\n",
    "\n",
    "attack = SimBA(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9437e",
   "metadata": {},
   "source": [
    "We set `num_queries = 0` using the `__reset__` method and run the attack, returning both `l2_norm` and `num_queries` . Then update the `optimize` call to include directions for _both_ return values, and Optuna will take care of the rest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021846c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def objective(trial, attack, x):\n",
    "    model_wrapper.__reset__()\n",
    "    new_params = {\n",
    "        \"max_iter\": trial.suggest_int('max_iter',  10, 3000),\n",
    "        \"epsilon\": trial.suggest_float('epsilon', 1e-6, 1.0),\n",
    "        \"freq_dim\": trial.suggest_int('freq_dim', 1, 20),\n",
    "        \"stride\": 1,\n",
    "        \"batch_size\": 1,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    attack.set_params(**new_params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = attack.generate(x = x)\n",
    "    l2_norm = torch.norm(img_tensor - results, p=2)\n",
    "    \n",
    "    num_queries = model_wrapper.num_queries\n",
    "    \n",
    "    return l2_norm, num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "study = optuna.create_study(directions=[\"minimize\", \"minimize\"])\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, attack=attack, x=img_tensor.numpy()), n_trials=100, show_progress_bar=True\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2cc4a",
   "metadata": {},
   "source": [
    "## Tiny Assessment (Optional Challenge)\n",
    "\n",
    ":::{exercise}\n",
    "Put it all together for a full assessment. We want to characterize the robustness of this model (or effectiveness of the attack) under different constraints for query budget.  Do the following.\n",
    "\n",
    "1. Rewrite the objective function so that it performs multiple trials (say 100), and computes the attack accuracy.\n",
    "2. Fix the query budget; optimize the remaining hyperparameters (`epsilon` and `freq_dim`) under the constrained query budget.\n",
    "3. Perform the optimization/estimation step for a range of query budgets (try 10, 50, 100, 200, 300, 400, 500, 1000)\n",
    "4. Visualize the results, showing the best attainable success rate of the attack against the query budget. Think through how you would report this to a data scientist.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e254dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eac52e",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab we optimized the optimizer! This is another favorite technique of ours - the framework defaults are _okay_, but they are by definition not the strongest possible attack. Each model is different; use Optuna to express that in your assessments. And don't forget to track which parameters work best against which targets!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "1. How to think about ML and ML security problems as optimizations.\n",
    "2. How to apply `Optuna` for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714679eb-482d-4a50-9d11-7a4968809510",
   "metadata": {},
   "source": [
    "**Move on to the [Inversion Module](../5_inversion/1_inversion_and_membership_inference.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cdc19a-4bfc-447d-9d1c-76f91bb4a376",
   "metadata": {},
   "source": [
    "![DLI Logo](../assets/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
